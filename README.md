# Local AI Assistant for IT Operations  
**Deploying a Self-Hosted LLM with Secure System Integration**

[![Python](https://img.shields.io/badge/Python-3.13.7-blue?logo=python&logoColor=white)](https://www.python.org/) 
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![Repo Status](https://img.shields.io/badge/Status-Phase%201-yellow)](README.md)

---

## Table of Contents

1. [Project Overview](#project-overview)  
2. [Features](#features)  
3. [Technology Stack](#technology-stack)  
4. [System Requirements](#system-requirements)  
5. [Step 1](#step-1)  
6. [Step 2](#Step-2)
7. [Step 3](#Step-3)  
8. [Notes](#notes)

---

## Project Overview

This project showcases my skills as an IT/Cybersecurity professional. The goal is to build a **self-hosted LLM** trained and prompt-tuned to assist with system administration tasks, including PowerShell scripting, log parsing, and local system management.  

The assistant will be able to:  
- Parse and answer IT-related questions (e.g., “What PowerShell command resets a user password?”)  
- Generate or review scripts for system tasks  
- Safely query local system information  
- Be accessed securely via a local **web interface** or **CLI**

---

## Features

- **Local LLM deployment**: Provides AI assistance without requiring an internet connection  
- **Integration with system utilities**: Interacts with Windows tools and scripts  
- **Flexible interface**: CLI and web-based access via terminal or browser  
- **Custom prompt tuning**: Specialized for IT and sysadmin tasks  

---

## Technology Stack

- **Ollama** – LLM backend  
- **Docker Desktop** – Containerization  
- **Python** – Scripting and automation 
- **FastAPI** – API layer framework  
- **Gradio** – Web-based user interface  

---

## System Requirements

- **Operating System:** Windows 11  
- **RAM:** 16 GB minimum (32 GB recommended)  
- **GPU:** 8 GB VRAM (preferably higher)  
- **Disk Space:** ~10–30 GB for models and cache  

---

## Step 1
Objectives:
- Install Ollama and Python environment

- Set up virtual environment

- Test LLM with simple prompts (test_ollama.py)

- Create initialization script to automatically start the virtual environment (start_assistant.ps1)

1. **Clone the repository**
```powershell
git clone https://github.com/kgandhi00-cmyk/LocalITAssistant.git
cd LocalITAssistant
```
2. **Setup Python virtual environment**
```powershell
python -m venv venv
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
.\venv\Scripts\Activate
```
3. **Install Dependencies**
```powershell
pip install -r requirements.txt
```
4. **Configure env variables**
Copy .env.example to .env & fill in required values (Ollama API key, etc)
```powershell
copy .env.example .env
```
5. **Test Ollama**
Verify response generated by Llama3
```powershell
python test_ollama.py
```
6. **Launch start assistant using PowerShell**
``` powershell
.\start_assistant.ps1
```

---

## Step 2
Objectives:
- Accepts IT questions and returns LLM answers,

- Can generate scripts (PowerShell/Bash) but won’t run them automatically,

- Can run a small set of predefined, safe read-only actions on your machine (e.g., list processes, list services),

- Provides a local HTTP API (FastAPI) to connect a UI or CLI,

- Logs queries/responses for debugging.

1. **Open project directory in powershell & start assistant **
```powershell
cd .\Documents\Projects\LocalITAssistant\
.\start_assistant.ps1
```
2. **Ask questions directly in powershell to verify functionality**
```powershell
python -m src.cli ask "How do I list running processes in PowerShell?"
```
3. **Launch assitant's back end (FastAPI)**
```powershell
 python -m uvicorn src.web:app --reload   
```
4. **In new powershell tab, enter test POST request (test API access)**
```powershell
(Invoke-RestMethod -Uri "http://127.0.0.1:8000/ask" -Method POST -Headers @{ "x-api-key"="example_api_key_please_replace" } -Body (@{ question="How do I list all running services in PowerShell?" } | ConvertTo-Json) -ContentType "application/json").answer
```

---

## Step 3
1. **Safe Action Whitelist**
Created safe_actions.json containing pre-approved commands (e.g., list processes, list services, disk usage).
Updated assistant.py to reference this file and run only whitelisted actions via a new run_safe method.

2. **CLI Improvements (cli.py)**
Added run-action command to safely execute whitelisted commands.
Updated list-actions to show all available safe actions.
JSON outputs from safe actions are automatically formatted for readability.

3. **Interactive CLI (cli_loop.py)**
New file src/cli_loop.py provides a REPL-style interface, allowing multiple questions and safe actions in a single session.
Maintains context between commands in the session for easier workflow.
Supports the following commands:
- ask <question> – Ask the assistant a question.
- gen-script <task> – Generate a PowerShell script for a task.
- list-actions – Show available safe actions.
- run-action <action> – Execute a safe action.
- show-log – Display session's chat history
- clear-memory – Clears session's chat history
- help – Show available commands.
- exit / quit – End the session.

**Usage:**
- List Actions
```powershell
python -m src.cli list-actions
```
- Run a safe action
```powershell
python -m src.cli run-action <action>
```
- Ask questions via CLI:
```powershell
python -m src.cli ask "<question>"
```
- Generate PowerShell script
```powershell
python -m src.cli gen-script <description>
```
*Can also use python -m src.cli_loop to enter mode that allows mulitple questions & commands in one session.*

---

## Notes

- Sensitive information (e.g., API keys) is stored in a `.env` file, **NOT in code**  
- The repository includes `.gitignore` to exclude virtual environments, model caches, and secrets  
- This project is intended for **local, private use only**
